{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b510c39",
   "metadata": {
    "papermill": {
     "duration": 0.005346,
     "end_time": "2022-11-16T07:24:21.567746",
     "exception": false,
     "start_time": "2022-11-16T07:24:21.562400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load feature and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbbfea-9f82-41af-8031-5882ed8ad2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from scipy.sparse import hstack,vstack,csr_matrix,save_npz,load_npz\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import json\n",
    "\n",
    "############################################################################\n",
    "#----- work folder -----\n",
    "############################################################################\n",
    "settings = json.load(open('./settings.json'))\n",
    "\n",
    "input_path = settings['input_path']\n",
    "features_path = settings['features_path']\n",
    "model_path = settings['model_path']\n",
    "sub_path = settings['sub_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed851392-b5a6-4f9c-859e-0de5cabe44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def zscore(x):\n",
    "    x_zscore = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_row = x[i]\n",
    "        x_row = (x_row - np.mean(x_row)) / np.std(x_row)\n",
    "        x_zscore.append(x_row)\n",
    "    x_std = np.array(x_zscore)    \n",
    "    return x_std\n",
    "\n",
    "print('target')\n",
    "train_df = pd.read_feather(feature_path+'train_multi_inputs_id.feather')\n",
    "test_df = pd.read_feather(feature_path+'test_multi_inputs_id.feather')\n",
    "train_multi_y = np.load(feature_path+'train_multi_targets.npy')    \n",
    "train_multi_y = zscore(train_multi_y)\n",
    "\n",
    "print('clr svd')\n",
    "train_df_raw = pd.read_feather(feature_path+'train_multi_inputs_id_raw.feather')\n",
    "\n",
    "multi_inputs_clr_svd = np.load(feature_path+'multi_inputs_svd_clr_100.npy')\n",
    "train_multi_clr_svd = multi_inputs_clr_svd[:len(train_df_raw)]\n",
    "train_multi_clr_svd = zscore(train_multi_clr_svd)\n",
    "\n",
    "train_multi_clr_svd = pd.DataFrame(train_multi_clr_svd)\n",
    "train_multi_clr_svd['cell_id'] = train_df_raw['cell_id']\n",
    "\n",
    "train_multi_clr_svd = train_df.merge(train_multi_clr_svd, on=['cell_id'], how='left')\n",
    "train_multi_clr_svd = train_multi_clr_svd.fillna(0)\n",
    "train_multi_clr_svd = train_multi_clr_svd.drop(['cell_id','day','donor','cell_type'],axis=1).values\n",
    "\n",
    "test_multi_clr_svd = multi_inputs_clr_svd[len(train_df_raw):]\n",
    "test_multi_clr_svd = zscore(test_multi_clr_svd)\n",
    "\n",
    "print('lgb1')\n",
    "multi_lgb1_svd = np.load(feature_path+'multi_lgb_svd_100.npy')\n",
    "train_multi_lgb1_svd = multi_lgb1_svd[:len(train_df)]\n",
    "train_multi_lgb1_svd = zscore(train_multi_lgb1_svd)\n",
    "\n",
    "test_multi_lgb1_svd = multi_lgb1_svd[len(train_df):]\n",
    "test_multi_lgb1_svd = zscore(test_multi_lgb1_svd)\n",
    "\n",
    "print('concatenate')\n",
    "train_multi_X = np.concatenate([train_multi_clr_svd,\n",
    "                                train_multi_lgb1_svd,  \n",
    "                                ],axis=1)\n",
    "\n",
    "test_multi_X = np.concatenate([test_multi_clr_svd,\n",
    "                                test_multi_lgb1_svd,\n",
    "                                ],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb76ffe",
   "metadata": {
    "papermill": {
     "duration": 0.003751,
     "end_time": "2022-11-16T07:24:28.467417",
     "exception": false,
     "start_time": "2022-11-16T07:24:28.463666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa8e0d",
   "metadata": {
    "papermill": {
     "duration": 6.531409,
     "end_time": "2022-11-16T07:24:35.003180",
     "exception": false,
     "start_time": "2022-11-16T07:24:28.471771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def correlation_score(y_true, y_pred):\n",
    "    \"\"\"Scores the predictions according to the competition rules. \n",
    "    \n",
    "    It is assumed that the predictions are not constant.\n",
    "    \n",
    "    Returns the average of each sample's Pearson correlation coefficient\"\"\"\n",
    "    if type(y_true) == pd.DataFrame: y_true = y_true.values\n",
    "    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)\n",
    "\n",
    "def cosine_similarity_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = tf.reduce_mean(x, axis=1, keepdims=True)\n",
    "    my = tf.reduce_mean(y, axis=1, keepdims=True)\n",
    "    xm, ym = x - mx, y - my\n",
    "    t1_norm = tf.math.l2_normalize(xm, axis = 1)\n",
    "    t2_norm = tf.math.l2_normalize(ym, axis = 1)\n",
    "    cosine = tf.keras.losses.CosineSimilarity(axis = 1)(t1_norm, t2_norm)\n",
    "    return cosine\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, train_X, train_y, list_IDs, shuffle, batch_size, labels, ): \n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.list_IDs = list_IDs        \n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        ct = len(self.list_IDs) // self.batch_size\n",
    "        return ct\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.list_IDs[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "    \n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        if self.labels: return X, y\n",
    "        else: return X\n",
    " \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange( len(self.list_IDs) )\n",
    "        if self.shuffle: \n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'    \n",
    "        X = self.train_X[list_IDs_temp]\n",
    "        y = self.train_y[list_IDs_temp]        \n",
    "        return X, y\n",
    "    \n",
    "def nn_kfold(train_df, train_multi_X, train_multi_y, test_df, test_multi_X, network, folds, model_name):\n",
    "    oof_preds = np.zeros((train_df.shape[0],23418))\n",
    "    sub_preds = np.zeros((test_df.shape[0],23418))\n",
    "    cv_corr = []\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df,)):          \n",
    "        print (n_fold)\n",
    "        train_x = train_multi_X[train_idx]\n",
    "        valid_x = train_multi_X[valid_idx]\n",
    "        train_y = train_multi_y[train_idx]\n",
    "        valid_y = train_multi_y[valid_idx]\n",
    "\n",
    "        train_x_index = train_df.iloc[train_idx].reset_index(drop=True).index\n",
    "        valid_x_index = train_df.iloc[valid_idx].reset_index(drop=True).index\n",
    "        \n",
    "        model = network(train_multi_X.shape[1])\n",
    "        filepath = model_name+'_'+str(n_fold)+'.h5'\n",
    "        es = tf.keras.callbacks.EarlyStopping(patience=8, mode='min', verbose=1) \n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(monitor='val_loss', filepath=filepath, save_best_only=True,save_weights_only=True,mode='min') \n",
    "        reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=LR_FACTOR, patience=5, verbose=1)\n",
    "    \n",
    "        train_dataset = DataGenerator(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            list_IDs=train_x_index, \n",
    "            shuffle=True, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            labels=True,\n",
    "        )\n",
    "        \n",
    "        valid_dataset = DataGenerator(\n",
    "            valid_x,\n",
    "            valid_y,\n",
    "            list_IDs=valid_x_index, \n",
    "            shuffle=False, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            labels=True,\n",
    "        )\n",
    "    \n",
    "        hist = model.fit(train_dataset,\n",
    "                        validation_data=valid_dataset,  \n",
    "                        epochs=EPOCHS, \n",
    "                        callbacks=[checkpoint,es,reduce_lr_loss],\n",
    "                        workers=4,\n",
    "                        verbose=1)  \n",
    "    \n",
    "        model.load_weights(filepath)\n",
    "        \n",
    "        oof_preds[valid_idx] = model.predict(valid_x, \n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                verbose=1)\n",
    "        \n",
    "        oof_corr = correlation_score(valid_y,  oof_preds[valid_idx])\n",
    "        cv_corr.append(oof_corr)\n",
    "        print (cv_corr)       \n",
    "        \n",
    "        sub_preds += model.predict(test_multi_X, \n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                verbose=1) / folds.n_splits \n",
    "            \n",
    "        del model\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()    \n",
    "    cv = correlation_score(train_multi_y,  oof_preds)\n",
    "    print ('Overall:',cv)           \n",
    "    return oof_preds,sub_preds    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c62e2e",
   "metadata": {
    "papermill": {
     "duration": 0.004179,
     "end_time": "2022-11-16T07:24:35.012008",
     "exception": false,
     "start_time": "2022-11-16T07:24:35.007829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model1 - cosine similarity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520200d",
   "metadata": {
    "papermill": {
     "duration": 946.742451,
     "end_time": "2022-11-16T07:40:21.758845",
     "exception": false,
     "start_time": "2022-11-16T07:24:35.016394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def multi_cos_sim_model(len_num):\n",
    "    \n",
    "    #######################  svd  #######################   \n",
    "    input_num = tf.keras.Input(shape=(len_num))     \n",
    "    \n",
    "    x = tf.keras.layers.Dense(600,activation ='swish',)(input_num)    \n",
    "    x = tf.keras.layers.GaussianDropout(0.3)(x)   \n",
    "    x = tf.keras.layers.Dense(600,activation ='swish',)(x) \n",
    "    x = tf.keras.layers.GaussianDropout(0.3)(x)   \n",
    "    x = tf.keras.layers.Dense(600,activation ='swish',)(x) \n",
    "    x = tf.keras.layers.GaussianDropout(0.3)(x)    \n",
    "    \n",
    "    output = tf.keras.layers.Dense(23418, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(input_num, output)\n",
    "    lr=0.001\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=None, )\n",
    "    model.compile(loss=cosine_similarity_loss, optimizer=adam,)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "BATCH_SIZE = 150\n",
    "EPOCHS = 100\n",
    "LR_FACTOR = 0.1\n",
    "SEED = 666\n",
    "N_FOLD = 5\n",
    "folds = KFold(n_splits= N_FOLD, shuffle=True, random_state=SEED)     \n",
    "oof_preds_cos,sub_preds_cos = nn_kfold(train_df, train_multi_X, train_multi_y,test_df, test_multi_X, multi_cos_sim_model, folds, 'multi_cos_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5266cc6",
   "metadata": {
    "papermill": {
     "duration": 0.65494,
     "end_time": "2022-11-16T07:40:23.000258",
     "exception": false,
     "start_time": "2022-11-16T07:40:22.345318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model2 - huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aed59a",
   "metadata": {
    "papermill": {
     "duration": 755.460186,
     "end_time": "2022-11-16T07:52:59.055473",
     "exception": false,
     "start_time": "2022-11-16T07:40:23.595287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def multi_huber_model(len_num):\n",
    "    \n",
    "    #######################  svd  #######################   \n",
    "    input_num = tf.keras.Input(shape=(len_num))     \n",
    "\n",
    "    x = tf.keras.layers.Dense(500,activation ='swish',)(input_num)    \n",
    "    x = tf.keras.layers.GaussianDropout(0.3)(x)   \n",
    "    x = tf.keras.layers.Dense(500,activation ='swish',)(x) \n",
    "    x = tf.keras.layers.GaussianDropout(0.3)(x)   \n",
    "    x = tf.keras.layers.Dense(500,activation ='swish',)(x) \n",
    "    x = tf.keras.layers.GaussianDropout(0.3)(x)    \n",
    "    \n",
    "    output = tf.keras.layers.Dense(23418, activation='linear')(x) \n",
    "\n",
    "    model = tf.keras.models.Model(input_num, output)    \n",
    "    \n",
    "    lr=0.001\n",
    "    decay = lr / 10\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay)\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.Huber(delta=4.0), optimizer=adam,)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "BATCH_SIZE = 150\n",
    "EPOCHS = 100\n",
    "LR_FACTOR = 0.1\n",
    "SEED = 666\n",
    "folds = KFold(n_splits= 5, shuffle=True, random_state=SEED)    \n",
    "\n",
    "oof_preds_huber,sub_preds_huber = nn_kfold(train_df, train_multi_X, train_multi_y,test_df, test_multi_X, multi_huber_model, folds, 'multi_huber_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0335bb",
   "metadata": {
    "papermill": {
     "duration": 1.203416,
     "end_time": "2022-11-16T07:53:01.456270",
     "exception": false,
     "start_time": "2022-11-16T07:53:00.252854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1242f",
   "metadata": {
    "papermill": {
     "duration": 22.234165,
     "end_time": "2022-11-16T07:53:24.906110",
     "exception": false,
     "start_time": "2022-11-16T07:53:02.671945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "oof_preds_cos = zscore(oof_preds_cos)\n",
    "oof_preds_huber = zscore(oof_preds_huber)\n",
    "oof_preds = oof_preds_cos*0.5 + oof_preds_huber*0.5\n",
    "cv = correlation_score(train_cite_y,  oof_preds)\n",
    "print ('Blend:',cv)     \n",
    "\n",
    "sub_preds_cos = zscore(sub_preds_cos)\n",
    "sub_preds_huber = zscore(sub_preds_huber)\n",
    "sub_preds = sub_preds_cos*0.5 + sub_preds_huber*0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b8997-4649-473a-8409-1111ed2b78df",
   "metadata": {},
   "source": [
    "# Save for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de385a-faa3-4450-91d3-cf18d3b16188",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(ensemble_path+'senkin_multi_ensemble.npy', sub_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de797682-3c55-4006-a702-31fdeb88ae6f",
   "metadata": {},
   "source": [
    "# Merge Cite and Multi  submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585dc51-fb63-4ab4-8f93-70814c174d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(input_path+'metadata.csv.zip')[['cell_id','technology']]\n",
    "evaluation_ids = pd.read_csv(input_path+'evaluation_ids.csv.zip')\n",
    "evaluation_ids = evaluation_ids.merge(metadata, on=['cell_id'], how='left')\n",
    "\n",
    "# cite\n",
    "train_cite_targets = pd.read_hdf(input_path+'train_cite_targets.h5')\n",
    "cite_targets = train_cite_targets.columns.values.tolist()\n",
    "\n",
    "del train_cite_targets\n",
    "gc.collect()\n",
    "\n",
    "test_preds_cite = np.load(ensemble_path+'cite_sub_preds.npy')\n",
    "test_preds_cite = pd.DataFrame(test_preds_cite, columns=cite_targets)\n",
    "\n",
    "test_cite_inputs_id = pd.read_feather(feature_path+'test_cite_inputs_id.feather')\n",
    "test_preds_cite['cell_id'] = test_cite_inputs_id['cell_id']\n",
    "test_preds_cite = test_preds_cite[test_preds_cite['cell_id'].isin(evaluation_ids['cell_id'])]\n",
    "test_preds_cite = pd.melt(test_preds_cite,id_vars='cell_id')\n",
    "test_preds_cite.columns = ['cell_id','gene_id','target']\n",
    "\n",
    "del test_cite_inputs_id\n",
    "gc.collect()\n",
    "\n",
    "# multi\n",
    "train_multi_targets = pd.read_hdf(input_path+'train_multi_targets.h5')\n",
    "multi_targets = train_multi_targets.columns.values.tolist()\n",
    "\n",
    "del train_multi_targets\n",
    "gc.collect()\n",
    "\n",
    "test_preds_multi = pd.DataFrame(sub_preds, columns=multi_targets)\n",
    "\n",
    "test_multi_inputs_id = pd.read_feather(feature_path+'test_multi_inputs_id.feather')\n",
    "test_preds_multi['cell_id'] = test_multi_inputs_id['cell_id']\n",
    "test_preds_multi = test_preds_multi[test_preds_multi['cell_id'].isin(evaluation_ids['cell_id'])]\n",
    "test_preds_multi = pd.melt(test_preds_multi,id_vars='cell_id')\n",
    "test_preds_multi.columns = ['cell_id','gene_id','target']\n",
    "\n",
    "del test_multi_inputs_id\n",
    "gc.collect()\n",
    "\n",
    "# merge\n",
    "test_preds = pd.concat([test_preds_cite,test_preds_multi])\n",
    "evaluation_ids = evaluation_ids.merge(test_preds, on=['cell_id','gene_id'], how='left')\n",
    "evaluation_ids[['row_id','target']].to_csv(sub_path+'submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e40a3",
   "metadata": {
    "papermill": {
     "duration": 1.202659,
     "end_time": "2022-11-16T07:56:36.174976",
     "exception": false,
     "start_time": "2022-11-16T07:56:34.972317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1947.418042,
   "end_time": "2022-11-16T07:56:40.432467",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-16T07:24:13.014425",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
