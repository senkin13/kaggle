{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b510c39",
   "metadata": {
    "papermill": {
     "duration": 0.005346,
     "end_time": "2022-11-16T07:24:21.567746",
     "exception": false,
     "start_time": "2022-11-16T07:24:21.562400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load feature and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbbfea-9f82-41af-8031-5882ed8ad2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from scipy.sparse import hstack,vstack,csr_matrix,save_npz,load_npz\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import json\n",
    "\n",
    "############################################################################\n",
    "#----- work folder -----\n",
    "############################################################################\n",
    "settings = json.load(open('./settings.json'))\n",
    "\n",
    "input_path = settings['input_path']\n",
    "features_path = settings['features_path']\n",
    "model_path = settings['model_path']\n",
    "sub_path = settings['sub_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed851392-b5a6-4f9c-859e-0de5cabe44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def zscore(x):\n",
    "    x_zscore = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_row = x[i]\n",
    "        x_row = (x_row - np.mean(x_row)) / np.std(x_row)\n",
    "        x_zscore.append(x_row)\n",
    "    x_std = np.array(x_zscore)    \n",
    "    return x_std\n",
    "\n",
    "print('modify test id')\n",
    "test_cite_inputs = pd.read_hdf(input_path+'test_cite_inputs.h5').reset_index()[['cell_id']]\n",
    "test_cite_inputs_raw = pd.read_hdf(input_path+'test_cite_inputs_raw.h5').reset_index()\n",
    "\n",
    "train_df = pd.read_feather(feature_path+'train_cite_inputs_id.feather')\n",
    "test_df = pd.read_feather(feature_path+'test_cite_inputs_id.feather')\n",
    "\n",
    "print('target')\n",
    "train_cite_y = np.load(feature_path+'train_cite_targets.npy')    \n",
    "\n",
    "print('cite_inputs_svd_clr')\n",
    "cite_inputs_svd_clr = np.load(feature_path+'cite_inputs_svd_clr_200.npy')\n",
    "train_cite_svd_clr = cite_inputs_svd_clr[:len(train_df)]\n",
    "test_cite_svd_clr = cite_inputs_svd_clr[len(train_df):]\n",
    "train_cite_svd_clr = zscore(train_cite_svd_clr)\n",
    "test_cite_svd_clr = zscore(test_cite_svd_clr)\n",
    "\n",
    "df_test_cite_svd_clr = pd.DataFrame(test_cite_svd_clr)\n",
    "df_test_cite_svd_clr['cell_id'] = test_cite_inputs_raw['cell_id']\n",
    "test_cite_inputs_id = test_cite_inputs.copy()\n",
    "test_cite_inputs_id = test_cite_inputs_id.merge(df_test_cite_svd_clr, on=['cell_id'], how='left')\n",
    "test_cite_inputs_id = test_cite_inputs_id.fillna(0)\n",
    "test_cite_svd_clr = test_cite_inputs_id.drop(['cell_id'],axis=1).values\n",
    "\n",
    "print('cite_inputs_bio_norm')\n",
    "cite_inputs_bio_norm_svd = np.load(feature_path+'cite_inputs_bio_norm_svd_100.npy')\n",
    "train_cite_inputs_bio_norm_svd = cite_inputs_bio_norm_svd[:len(train_df)]\n",
    "test_cite_inputs_bio_norm_svd = cite_inputs_bio_norm_svd[len(train_df):]\n",
    "train_cite_inputs_bio_norm_svd = zscore(train_cite_inputs_bio_norm_svd)\n",
    "test_cite_inputs_bio_norm_svd = zscore(test_cite_inputs_bio_norm_svd)\n",
    "\n",
    "df_test_cite_inputs_bio_norm_svd = pd.DataFrame(test_cite_inputs_bio_norm_svd)\n",
    "df_test_cite_inputs_bio_norm_svd['cell_id'] = test_cite_inputs_raw['cell_id']\n",
    "test_cite_inputs_id = test_cite_inputs.copy()\n",
    "test_cite_inputs_id = test_cite_inputs_id.merge(df_test_cite_inputs_bio_norm_svd, on=['cell_id'], how='left')\n",
    "test_cite_inputs_id = test_cite_inputs_id.fillna(0)\n",
    "test_cite_inputs_bio_norm_svd = test_cite_inputs_id.drop(['cell_id'],axis=1).values\n",
    "\n",
    "print('cite_inputs_raw_important_feats')\n",
    "cite_inputs_feats = np.load(feature_path+'cite_inputs_raw_important_feats.npy') \n",
    "train_cite_inputs_feats = cite_inputs_feats[:len(train_df)]\n",
    "test_cite_inputs_feats = cite_inputs_feats[len(train_df):]\n",
    "train_cite_inputs_feats = zscore(train_cite_inputs_feats)\n",
    "test_cite_inputs_feats = zscore(test_cite_inputs_feats)\n",
    "\n",
    "df_test_cite_inputs_feats = pd.DataFrame(test_cite_inputs_feats)\n",
    "df_test_cite_inputs_feats['cell_id'] = test_cite_inputs_raw['cell_id']\n",
    "test_cite_inputs_id = test_cite_inputs.copy()\n",
    "test_cite_inputs_id = test_cite_inputs_id.merge(df_test_cite_inputs_feats, on=['cell_id'], how='left')\n",
    "test_cite_inputs_id = test_cite_inputs_id.fillna(0)\n",
    "test_cite_inputs_feats = test_cite_inputs_id.drop(['cell_id'],axis=1).values\n",
    "\n",
    "print('cite_inputs_bio_norm_pca_64')\n",
    "cite_inputs_bio_norm_pca_64 = np.load(feature_path+'cite_inputs_bio_norm_pca_64.npy')\n",
    "train_cite_inputs_bio_norm_pca_64 = cite_inputs_bio_norm_pca_64[:len(train_df)]\n",
    "test_cite_inputs_bio_norm_pca_64 = cite_inputs_bio_norm_pca_64[len(train_df):]\n",
    "train_cite_inputs_bio_norm_pca_64 = zscore(train_cite_inputs_bio_norm_pca_64)\n",
    "test_cite_inputs_bio_norm_pca_64 = zscore(test_cite_inputs_bio_norm_pca_64)\n",
    "\n",
    "df_test_cite_inputs_bio_norm_pca_64 = pd.DataFrame(test_cite_inputs_bio_norm_pca_64)\n",
    "df_test_cite_inputs_bio_norm_pca_64['cell_id'] = test_cite_inputs_raw['cell_id']\n",
    "test_cite_inputs_id = test_cite_inputs.copy()\n",
    "test_cite_inputs_id = test_cite_inputs_id.merge(df_test_cite_inputs_bio_norm_pca_64, on=['cell_id'], how='left')\n",
    "test_cite_inputs_id = test_cite_inputs_id.fillna(0)\n",
    "test_cite_inputs_bio_norm_pca_64 = test_cite_inputs_id.drop(['cell_id'],axis=1).values\n",
    "\n",
    "\n",
    "print('lgb1')\n",
    "cite_lgb1_svd = np.load(feature_path+'cite_lgb1_svd_100.npy')\n",
    "train_cite_lgb1_svd = cite_lgb1_svd[:len(train_df)]\n",
    "test_cite_lgb1_svd = cite_lgb1_svd[len(train_df):]\n",
    "train_cite_lgb1_svd = zscore(train_cite_lgb1_svd)\n",
    "test_cite_lgb1_svd = zscore(test_cite_lgb1_svd)\n",
    "\n",
    "\n",
    "print('lgb2')\n",
    "cite_lgb2_svd = np.load(feature_path+'cite_lgb2_svd_100.npy')\n",
    "train_cite_lgb2_svd = cite_lgb2_svd[:len(train_df)]\n",
    "test_cite_lgb2_svd = cite_lgb2_svd[len(train_df):]\n",
    "train_cite_lgb2_svd = zscore(train_cite_lgb2_svd)\n",
    "test_cite_lgb2_svd = zscore(test_cite_lgb2_svd)\n",
    "\n",
    "\n",
    "print('lgb3')\n",
    "cite_lgb3_svd = np.load(feature_path+'cite_lgb3_svd_100.npy')\n",
    "train_cite_lgb3_svd = cite_lgb3_svd[:len(train_df)]\n",
    "test_cite_lgb3_svd = cite_lgb3_svd[len(train_df):]\n",
    "train_cite_lgb3_svd = zscore(train_cite_lgb3_svd)\n",
    "test_cite_lgb3_svd = zscore(test_cite_lgb3_svd)\n",
    "\n",
    "df_test_cite_lgb3_svd = pd.DataFrame(test_cite_lgb3_svd)\n",
    "df_test_cite_lgb3_svd['cell_id'] = test_cite_inputs_raw['cell_id']\n",
    "test_cite_inputs_id = test_cite_inputs.copy()\n",
    "test_cite_inputs_id = test_cite_inputs_id.merge(df_test_cite_lgb3_svd, on=['cell_id'], how='left')\n",
    "test_cite_inputs_id = test_cite_inputs_id.fillna(0)\n",
    "test_cite_lgb3_svd = test_cite_inputs_id.drop(['cell_id'],axis=1).values\n",
    "\n",
    "print('lgb4')\n",
    "cite_lgb4_svd = np.load(feature_path+'cite_lgb4_svd_100.npy')\n",
    "train_cite_lgb4_svd = cite_lgb4_svd[:len(train_df)]\n",
    "test_cite_lgb4_svd = cite_lgb4_svd[len(train_df):]\n",
    "train_cite_lgb4_svd = zscore(train_cite_lgb4_svd)\n",
    "test_cite_lgb4_svd = zscore(test_cite_lgb4_svd)\n",
    "\n",
    "df_test_cite_lgb4_svd = pd.DataFrame(test_cite_lgb4_svd)\n",
    "df_test_cite_lgb4_svd['cell_id'] = test_cite_inputs_raw['cell_id']\n",
    "test_cite_inputs_id = test_cite_inputs.copy()\n",
    "test_cite_inputs_id = test_cite_inputs_id.merge(df_test_cite_lgb4_svd, on=['cell_id'], how='left')\n",
    "test_cite_inputs_id = test_cite_inputs_id.fillna(0)\n",
    "test_cite_lgb4_svd = test_cite_inputs_id.drop(['cell_id'],axis=1).values\n",
    "\n",
    "print('concatenate')\n",
    "train_cite_X = np.concatenate([\n",
    "                               train_cite_svd_clr,\n",
    "                               train_cite_inputs_feats,\n",
    "                               train_cite_inputs_bio_norm_svd,\n",
    "                               train_cite_inputs_bio_norm_pca_64,\n",
    "                               train_cite_lgb1_svd,\n",
    "                               train_cite_lgb2_svd,\n",
    "                               train_cite_lgb3_svd,\n",
    "                               train_cite_lgb4_svd,\n",
    "                                ],axis=1)\n",
    "\n",
    "test_cite_X = np.concatenate([\n",
    "                              test_cite_svd_clr,\n",
    "                              test_cite_inputs_feats,\n",
    "                              test_cite_inputs_bio_norm_svd, \n",
    "                              test_cite_inputs_bio_norm_pca_64,\n",
    "                              test_cite_lgb1_svd,\n",
    "                              test_cite_lgb2_svd,\n",
    "                              test_cite_lgb3_svd,\n",
    "                              test_cite_lgb4_svd,\n",
    "                                ],axis=1)\n",
    "\n",
    "train_cite_y = np.load(feature_path+'train_cite_targets.npy')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bcb4fc",
   "metadata": {
    "papermill": {
     "duration": 0.016762,
     "end_time": "2022-11-16T07:24:28.459805",
     "exception": false,
     "start_time": "2022-11-16T07:24:28.443043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.shape,test_df.shape,train_cite_X.shape,test_cite_X.shape,train_cite_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb76ffe",
   "metadata": {
    "papermill": {
     "duration": 0.003751,
     "end_time": "2022-11-16T07:24:28.467417",
     "exception": false,
     "start_time": "2022-11-16T07:24:28.463666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa8e0d",
   "metadata": {
    "papermill": {
     "duration": 6.531409,
     "end_time": "2022-11-16T07:24:35.003180",
     "exception": false,
     "start_time": "2022-11-16T07:24:28.471771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def correlation_score(y_true, y_pred):\n",
    "    \"\"\"Scores the predictions according to the competition rules. \n",
    "    \n",
    "    It is assumed that the predictions are not constant.\n",
    "    \n",
    "    Returns the average of each sample's Pearson correlation coefficient\"\"\"\n",
    "    if type(y_true) == pd.DataFrame: y_true = y_true.values\n",
    "    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)\n",
    "\n",
    "def zscore(x):\n",
    "    x_zscore = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_row = x[i]\n",
    "        x_row = (x_row - np.mean(x_row)) / np.std(x_row)\n",
    "        x_zscore.append(x_row)\n",
    "    x_std = np.array(x_zscore)    \n",
    "    return x_std\n",
    "\n",
    "def cosine_similarity_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = tf.reduce_mean(x, axis=1, keepdims=True)\n",
    "    my = tf.reduce_mean(y, axis=1, keepdims=True)\n",
    "    xm, ym = x - mx, y - my\n",
    "    t1_norm = tf.math.l2_normalize(xm, axis = 1)\n",
    "    t2_norm = tf.math.l2_normalize(ym, axis = 1)\n",
    "    cosine = tf.keras.losses.CosineSimilarity(axis = 1)(t1_norm, t2_norm)\n",
    "    return cosine\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, train_X, train_y, list_IDs, shuffle, batch_size, labels, ): \n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.list_IDs = list_IDs        \n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        ct = len(self.list_IDs) // self.batch_size\n",
    "        return ct\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.list_IDs[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "    \n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        if self.labels: return X, y\n",
    "        else: return X\n",
    " \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange( len(self.list_IDs) )\n",
    "        if self.shuffle: \n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'    \n",
    "        X = self.train_X[list_IDs_temp]\n",
    "        y = self.train_y[list_IDs_temp]        \n",
    "        return X, y\n",
    "    \n",
    "def nn_kfold(train_df, train_cite_X, train_cite_y, test_df, test_cite_X, network, folds, model_name):\n",
    "    oof_preds = np.zeros((train_df.shape[0],140))\n",
    "    sub_preds = np.zeros((test_df.shape[0],140))\n",
    "    cv_corr = []\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df,)):          \n",
    "        print (n_fold)\n",
    "        train_x = train_cite_X[train_idx]\n",
    "        valid_x = train_cite_X[valid_idx]\n",
    "        train_y = train_cite_y[train_idx]\n",
    "        valid_y = train_cite_y[valid_idx]\n",
    "\n",
    "        train_x_index = train_df.iloc[train_idx].reset_index(drop=True).index\n",
    "        valid_x_index = train_df.iloc[valid_idx].reset_index(drop=True).index\n",
    "        \n",
    "        model = network(train_cite_X.shape[1])\n",
    "        filepath = model_name+'_'+str(n_fold)+'.h5'\n",
    "        es = tf.keras.callbacks.EarlyStopping(patience=10, mode='min', verbose=1) \n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(monitor='val_loss', filepath=filepath, save_best_only=True,save_weights_only=True,mode='min') \n",
    "        reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=LR_FACTOR, patience=6, verbose=1)\n",
    "    \n",
    "        train_dataset = DataGenerator(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            list_IDs=train_x_index, \n",
    "            shuffle=True, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            labels=True,\n",
    "        )\n",
    "        \n",
    "        valid_dataset = DataGenerator(\n",
    "            valid_x,\n",
    "            valid_y,\n",
    "            list_IDs=valid_x_index, \n",
    "            shuffle=False, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            labels=True,\n",
    "        )\n",
    "    \n",
    "        hist = model.fit(train_dataset,\n",
    "                        validation_data=valid_dataset,  \n",
    "                        epochs=EPOCHS, \n",
    "                        callbacks=[checkpoint,es,reduce_lr_loss],\n",
    "                        workers=4,\n",
    "                        verbose=1)  \n",
    "    \n",
    "        model.load_weights(filepath)\n",
    "        \n",
    "        oof_preds[valid_idx] = model.predict(valid_x, \n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                verbose=1)\n",
    "        \n",
    "        oof_corr = correlation_score(valid_y,  oof_preds[valid_idx])\n",
    "        cv_corr.append(oof_corr)\n",
    "        print (cv_corr)       \n",
    "        \n",
    "        sub_preds += model.predict(test_cite_X, \n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                verbose=1) / folds.n_splits \n",
    "            \n",
    "        del model\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()    \n",
    "    cv = correlation_score(train_cite_y,  oof_preds)\n",
    "    print ('Overall:',cv)           \n",
    "    return oof_preds,sub_preds    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c62e2e",
   "metadata": {
    "papermill": {
     "duration": 0.004179,
     "end_time": "2022-11-16T07:24:35.012008",
     "exception": false,
     "start_time": "2022-11-16T07:24:35.007829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model1 - cosine similarity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520200d",
   "metadata": {
    "papermill": {
     "duration": 946.742451,
     "end_time": "2022-11-16T07:40:21.758845",
     "exception": false,
     "start_time": "2022-11-16T07:24:35.016394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def cite_cos_sim_model(len_num):\n",
    "    \n",
    "    #######################  svd  #######################   \n",
    "    input_num = tf.keras.Input(shape=(len_num))     \n",
    "    x = input_num\n",
    "    x0 =  tf.keras.layers.Reshape((1,x.shape[1]))(x)\n",
    "    x0 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(1800, activation='elu', kernel_initializer='Identity',return_sequences=False))(x0)\n",
    "    x1 = tf.keras.layers.GaussianDropout(0.2)(x0)         \n",
    "    x2 = tf.keras.layers.Dense(1800,activation ='elu',kernel_initializer='Identity',)(x1) \n",
    "    x3 = tf.keras.layers.GaussianDropout(0.2)(x2) \n",
    "    x4 = tf.keras.layers.Dense(1800,activation ='elu',kernel_initializer='Identity',)(x3) \n",
    "    x5 = tf.keras.layers.GaussianDropout(0.2)(x4)         \n",
    "    x = tf.keras.layers.Concatenate()([\n",
    "                       x1,x3,x5\n",
    "                      ])\n",
    "    output = tf.keras.layers.Dense(140, activation='linear')(x) \n",
    "    model = tf.keras.models.Model(input_num, output)\n",
    "    lr=0.001\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=None, )\n",
    "    model.compile(loss=cosine_similarity_loss, optimizer=adam,)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "BATCH_SIZE = 620\n",
    "EPOCHS = 100\n",
    "LR_FACTOR = 0.05\n",
    "SEED = 666\n",
    "N_FOLD = 5\n",
    "folds = KFold(n_splits= N_FOLD, shuffle=True, random_state=SEED)     \n",
    "oof_preds_cos,sub_preds_cos = nn_kfold(train_df, train_cite_X, train_cite_y,test_df, test_cite_X, cite_cos_sim_model, folds, 'cite_cos_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5266cc6",
   "metadata": {
    "papermill": {
     "duration": 0.65494,
     "end_time": "2022-11-16T07:40:23.000258",
     "exception": false,
     "start_time": "2022-11-16T07:40:22.345318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model2 - mse loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aed59a",
   "metadata": {
    "papermill": {
     "duration": 755.460186,
     "end_time": "2022-11-16T07:52:59.055473",
     "exception": false,
     "start_time": "2022-11-16T07:40:23.595287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def cite_mse_model(len_num):\n",
    "    \n",
    "    #######################  svd  #######################   \n",
    "    input_num = tf.keras.Input(shape=(len_num))     \n",
    "\n",
    "    x = input_num\n",
    "    x = tf.keras.layers.Dense(1500,activation ='swish',)(x)    \n",
    "    x = tf.keras.layers.GaussianDropout(0.1)(x)   \n",
    "    x = tf.keras.layers.Dense(1500,activation ='swish',)(x) \n",
    "    x = tf.keras.layers.GaussianDropout(0.1)(x)   \n",
    "    x = tf.keras.layers.Dense(1500,activation ='swish',)(x) \n",
    "    x = tf.keras.layers.GaussianDropout(0.1)(x)    \n",
    "    x =  tf.keras.layers.Reshape((1,x.shape[1]))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(700, activation='swish',return_sequences=False))(x)\n",
    "    x = tf.keras.layers.GaussianDropout(0.1)(x)  \n",
    "    \n",
    "    output = tf.keras.layers.Dense(140, activation='linear')(x) \n",
    "\n",
    "    model = tf.keras.models.Model(input_num, output)\n",
    "    \n",
    "    lr=0.0005\n",
    "    weight_decay = 0.0001\n",
    "    \n",
    "    opt = tfa.optimizers.AdamW(\n",
    "        learning_rate=lr, weight_decay=weight_decay\n",
    "    )    \n",
    "\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt,)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "BATCH_SIZE = 600\n",
    "EPOCHS = 100\n",
    "LR_FACTOR = 0.1\n",
    "SEED = 666\n",
    "folds = KFold(n_splits= 5, shuffle=True, random_state=SEED)    \n",
    "\n",
    "# zscore for target\n",
    "train_cite_y = zscore(train_cite_y)\n",
    "\n",
    "oof_preds_mse,sub_preds_mse = nn_kfold(train_df, train_cite_X, train_cite_y, test_df, test_cite_X, cite_mse_model, folds, 'cite_mse_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0335bb",
   "metadata": {
    "papermill": {
     "duration": 1.203416,
     "end_time": "2022-11-16T07:53:01.456270",
     "exception": false,
     "start_time": "2022-11-16T07:53:00.252854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1242f",
   "metadata": {
    "papermill": {
     "duration": 22.234165,
     "end_time": "2022-11-16T07:53:24.906110",
     "exception": false,
     "start_time": "2022-11-16T07:53:02.671945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "oof_preds_cos = zscore(oof_preds_cos)\n",
    "oof_preds_mse = zscore(oof_preds_mse)\n",
    "oof_preds = oof_preds_cos*0.55 + oof_preds_mse*0.45\n",
    "cv = correlation_score(train_cite_y,  oof_preds)\n",
    "print ('Blend:',cv)     \n",
    "\n",
    "sub_preds_cos = zscore(sub_preds_cos)\n",
    "sub_preds_mse = zscore(sub_preds_mse)\n",
    "sub_preds = sub_preds_cos*0.55 + sub_preds_mse*0.45\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d55bfe-bdc4-4ed6-8c30-3d49605f2b96",
   "metadata": {},
   "source": [
    "# Save for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d64e0d-f145-4d0c-b8cb-b82b7e2fb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(ensemble_path+'senkin_cite_ensemble.npy', sub_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e40a3",
   "metadata": {
    "papermill": {
     "duration": 1.202659,
     "end_time": "2022-11-16T07:56:36.174976",
     "exception": false,
     "start_time": "2022-11-16T07:56:34.972317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1947.418042,
   "end_time": "2022-11-16T07:56:40.432467",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-16T07:24:13.014425",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
